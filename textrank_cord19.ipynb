{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\peeyu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def textrank(document):\n",
    "    # Tokenize the sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "\n",
    "    # Vectorize the sentences using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Create a graph where nodes are sentences\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[0]):\n",
    "            # Nodes are connected if they share similar words\n",
    "            if cosine_similarity(X[i], X[j]) > 0.2:\n",
    "                graph.add_edge(i, j)\n",
    "\n",
    "    # Rank the sentences\n",
    "    ranks = nx.pagerank(graph)\n",
    "    # print(graph)\n",
    "    # Sort the sentences by rank\n",
    "    # print(sorted(ranks.keys()), sentences[19])\n",
    "    ranked_sentences = sorted(((ranks[i], s) \n",
    "                               for i, s in enumerate(sentences) \n",
    "                               if i in ranks), reverse=True)\n",
    "\n",
    "    # Return the top 5 sentences as the summary\n",
    "    return ' '.join([s for rank, s in ranked_sentences[:min(5, len(sentences))]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"allenai/cord19\", \"fulltext\", \"abstract\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['cord_uid', 'sha', 'source_x', 'title', 'doi', 'abstract', 'publish_time', 'authors', 'journal', 'url', 'fulltext', 'select'],\n",
       "        num_rows: 105097\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def select_datapints(dataset):\n",
    "    dataset['select'] = (len(dataset['fulltext']) > 0) and (len(dataset['abstract']) > 0)\n",
    "    return dataset\n",
    "\n",
    "raw_datasets['train'] = raw_datasets['train'].map(select_datapints)\n",
    "indices_required = np.where(np.array(raw_datasets['train']['select']) == True)[0]\n",
    "raw_datasets['train'] = raw_datasets['train'].select(indices_required)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.remove_columns(['cord_uid', 'sha', 'source_x', 'title', 'doi', 'publish_time', 'authors', 'journal', 'url'])\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "from datasets import DatasetDict\n",
    "train_testvalid = raw_datasets['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Further split the test+validation into test and validation sets evenly\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "test_valid = test_valid['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "def get_rouge_scores(document):\n",
    "    import rouge\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    import networkx as nx\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    def textrank(document):\n",
    "        # Tokenize the sentences\n",
    "        sentences = sent_tokenize(document)\n",
    "\n",
    "        # Vectorize the sentences using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "        X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "        # Create a graph where nodes are sentences\n",
    "        graph = nx.Graph()\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(X.shape[0]):\n",
    "                # Nodes are connected if they share similar words\n",
    "                if cosine_similarity(X[i], X[j]) > 0.2:\n",
    "                    graph.add_edge(i, j)\n",
    "\n",
    "        # Rank the sentences\n",
    "        ranks = nx.pagerank(graph)\n",
    "        # print(graph)\n",
    "        # Sort the sentences by rank\n",
    "        # print(sorted(ranks.keys()), sentences[19])\n",
    "        ranked_sentences = sorted(((ranks[i], s) \n",
    "                                for i, s in enumerate(sentences) \n",
    "                                if i in ranks), reverse=True)\n",
    "\n",
    "        # Return the top 5 sentences as the summary\n",
    "        return ' '.join([s for rank, s in ranked_sentences[:min(5, len(sentences))]])\n",
    "\n",
    "\n",
    "    generated_text = textrank(document['fulltext'])\n",
    "    sentence_rouge_score = rouge.Rouge().get_scores(generated_text, \n",
    "                                                    document['abstract'])[0]\n",
    "    for rog in sentence_rouge_score:\n",
    "        document[rog] = sentence_rouge_score[rog]['f']\n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_valid.map(get_rouge_scores, num_proc=16)\n",
    "\n",
    "np.mean(test['rouge-1']), np.mean(test['rouge-2']), np.mean(test['rouge-l'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
